{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da3f277-9773-4af9-8c25-2860eb5062e1",
   "metadata": {},
   "source": [
    "# Detect animals, persons or vehicles in Camera Trap images\n",
    "\n",
    "Tools: Pytorch-Wildlife\n",
    "\n",
    "## Installation instructions:\n",
    "1. Create conda environment \n",
    "```\n",
    "conda create -n pytorch_wildlife python=3.8 -y \n",
    "conda activate pytorch_wildlife\n",
    "\n",
    "```\n",
    "2. Install using `pip install PytorchWildlife`\n",
    "\n",
    "## Detection model: MegaDetector V6\n",
    "\n",
    "MegaDetector repository: https://github.com/agentmorris/MegaDetector/tree/main\n",
    "\n",
    "MegaDetector package: https://pypi.org/project/megadetector/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e98fc0-1ec8-4e03-8a74-b255ac43a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n pytorch_wildlife python=3.8 -y\n",
    "# conda activate pytorch_wildlife\n",
    "# pip install PytorchWildlife\n",
    "# python3 -m ipykernel install --user --name pytorch_wildlife --display-name \"Python pytorch_wildlife\" # genera un kernel a partir del ambiente\n",
    "# Use the pytorch_wildlife environment kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c5c43-cfe4-4282-bd79-3abed4ebad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Detector PyTorchWildlife\n",
    "\n",
    "# --- Import packages ---\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from PytorchWildlife.models import detection as pw_detection\n",
    "from PytorchWildlife.models import classification as pw_classification\n",
    "from PytorchWildlife.data import transforms as pw_trans\n",
    "from PytorchWildlife.data import datasets as pw_data \n",
    "from PytorchWildlife import utils as pw_utils\n",
    "import wget\n",
    "import pandas as pd\n",
    "import supervision as sv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad0efb",
   "metadata": {},
   "source": [
    "Metricas de presici贸n del modelo entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8949ec70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom-classification-epoch</th>\n",
       "      <th>Custom-classification-lr-SGD/pg1</th>\n",
       "      <th>Custom-classification-lr-SGD/pg2</th>\n",
       "      <th>Custom-classification-train_loss</th>\n",
       "      <th>Custom-classification-valid_mac_acc</th>\n",
       "      <th>Custom-classification-valid_mic_acc</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.411263</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.271831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.296611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.279177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.530753</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.271027</td>\n",
       "      <td>1469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Custom-classification-epoch  Custom-classification-lr-SGD/pg1  \\\n",
       "314                         29.0                               NaN   \n",
       "316                         29.0                               NaN   \n",
       "318                         29.0                               NaN   \n",
       "320                         29.0                               NaN   \n",
       "322                         29.0                               NaN   \n",
       "323                         29.0                               NaN   \n",
       "\n",
       "     Custom-classification-lr-SGD/pg2  Custom-classification-train_loss  \\\n",
       "314                               NaN                          0.411263   \n",
       "316                               NaN                          0.271831   \n",
       "318                               NaN                          0.296611   \n",
       "320                               NaN                          0.279177   \n",
       "322                               NaN                          0.530753   \n",
       "323                               NaN                               NaN   \n",
       "\n",
       "     Custom-classification-valid_mac_acc  Custom-classification-valid_mic_acc  \\\n",
       "314                                  NaN                                  NaN   \n",
       "316                                  NaN                                  NaN   \n",
       "318                                  NaN                                  NaN   \n",
       "320                                  NaN                                  NaN   \n",
       "322                                  NaN                                  NaN   \n",
       "323                                  NaN                            53.271027   \n",
       "\n",
       "     step  \n",
       "314  1429  \n",
       "316  1439  \n",
       "318  1449  \n",
       "320  1459  \n",
       "322  1469  \n",
       "323  1469  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = pd.read_csv('finetunning_files/PW_FT_classification/log/logs/Plain/Plain_Crop_res50_plain_101224_test/version_0/metrics.csv')\n",
    "metrics[metrics['Custom-classification-epoch'] == 29.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a5041",
   "metadata": {},
   "source": [
    "Cargar clases incluidas en el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "464a2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0: 'AVES',\n",
    "    1: 'BASSSEISCUS_ASTUTUS',\n",
    "    2: 'CONEPATUS_LEUCONOTUS',\n",
    "    3: 'DIDELPHIS_VIRGINIATA',\n",
    "    4: 'GANADO',\n",
    "    5: 'LEOPARDU_wiedii',\n",
    "    6: 'LYNX_RUFUS',\n",
    "    7: 'MEPITIS_MACROURA',\n",
    "    8: 'NASUA_NARICA',\n",
    "    9: 'ODOCOILEUS_VIRGINIANUS',\n",
    "    10: 'PECARI_TAJACU',\n",
    "    11: 'PUMA_CONCOLOR',\n",
    "    12: 'SCIURUS_OCOLATUS',\n",
    "    13: 'SPILOGALE_GRACILIS',\n",
    "    14: 'SYLVILAGUS_SP',\n",
    "    15: 'UROCYON_CINEREORGENTEUS'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f0a15",
   "metadata": {},
   "source": [
    "Inicializar el modelo de detecci贸n (MegaDetectorV6) y el modelo de clasificaci贸n que entrenamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a15a22a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.48  Python-3.8.20 torch-2.4.1+cu121 CPU (Intel Xeon E5-2697 v4 2.30GHz)\n",
      "YOLOv9c summary (fused): 384 layers, 25,321,561 parameters, 0 gradients, 102.3 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "# --- Model initialization ---\n",
    "\n",
    "DEVICE = \"cpu\"  # Use \"cuda\" if GPU is available \"cpu\" if no GPU is available\n",
    "detection_model = pw_detection.MegaDetectorV6(device=DEVICE, pretrained=True, version=\"yolov9c\")\n",
    "classification_model = pw_classification.CustomWeights(weights='finetunning_files/PW_FT_classification/weights/logs/Plain/Crop_res50_plain_101224_test-0-epoch=14-valid_mac_acc=52.63.ckpt', class_names=class_names, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d49b0",
   "metadata": {},
   "source": [
    "### Identificaci贸n de lotes de imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706cd2ae-2c60-4e13-ba6a-15abbf96f000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/STORAGE/csar/pipo_images/PUMA_CONCOLOR_2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                        | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 animal, 426.7ms\n",
      "1: 640x640 1 animal, 426.7ms\n",
      "2: 640x640 1 animal, 426.7ms\n",
      "3: 640x640 1 animal, 426.7ms\n",
      "4: 640x640 1 animal, 426.7ms\n",
      "5: 640x640 1 animal, 426.7ms\n",
      "6: 640x640 1 animal, 426.7ms\n",
      "7: 640x640 1 animal, 426.7ms\n",
      "8: 640x640 1 animal, 426.7ms\n",
      "9: 640x640 1 animal, 426.7ms\n",
      "10: 640x640 1 animal, 426.7ms\n",
      "11: 640x640 1 animal, 426.7ms\n",
      "12: 640x640 1 animal, 426.7ms\n",
      "13: 640x640 2 animals, 426.7ms\n",
      "14: 640x640 2 animals, 426.7ms\n",
      "15: 640x640 1 animal, 426.7ms\n",
      "Speed: 3.4ms preprocess, 426.7ms inference, 0.5ms postprocess per image at shape (16, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|                                                                        | 1/2 [00:17<00:17, 17.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 1 animal, 328.3ms\n",
      "1: 512x640 2 animals, 328.3ms\n",
      "2: 512x640 1 animal, 328.3ms\n",
      "3: 512x640 1 animal, 328.3ms\n",
      "4: 512x640 1 animal, 328.3ms\n",
      "5: 512x640 1 animal, 328.3ms\n",
      "6: 512x640 1 animal, 328.3ms\n",
      "7: 512x640 1 animal, 328.3ms\n",
      "Speed: 2.7ms preprocess, 328.3ms inference, 1.1ms postprocess per image at shape (8, 3, 512, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|| 2/2 [00:24<00:00, 12.44s/it]"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "# --- Batch image detection ---\n",
    "\n",
    "tgt_folder_path = os.path.join(\"/mnt\", \"STORAGE\", \"csar\", \"pipo_images\", \"PUMA_CONCOLOR_2022\")\n",
    "results = detection_model.batch_image_detection(tgt_folder_path, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed0dfd-f1d4-4802-a4c7-c685ac9a28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_utils.save_detection_images(results, os.path.join(\"..\",\"output\",\"PUMA_CONCOLOR_2022\"), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db985d-0aff-45f8-a632-2ca27e6c46f0",
   "metadata": {},
   "source": [
    "### Detecci贸n y clasificaci贸n de videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eaa260-3318-4d52-a327-8c5914fd812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video detection\n",
    "SOURCE_VIDEO_PATH = os.path.join(\"/mnt\", \"STORAGE\", \"csar\", \"pipo_images\", \"PUMA_CONCOLOR_2022\", \"IMG_0004 (2).MP4\")\n",
    "TARGET_VIDEO_PATH =  os.path.join(\"..\",\"output\",\"videos\",\"PUMA_CONCOLOR_2022\",\"IMG_0004 (2).MP4\")\n",
    "box_annotator = sv.BoxAnnotator(thickness=4)\n",
    "lab_annotator = sv.LabelAnnotator(text_color=sv.Color.BLACK, text_thickness=4, text_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0fc13-9d1c-4a2c-8fa4-e84b4b377015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci贸n para detectar y clasificar frames en los videos.\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Callback function to process each video frame for detection and classification.\n",
    "    \n",
    "    Parameters:\n",
    "    - frame (np.ndarray): Video frame as a numpy array.\n",
    "    - index (int): Frame index.\n",
    "    \n",
    "    Returns:\n",
    "    annotated_frame (np.ndarray): Annotated video frame.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Detect objects in the frame using the detection model.\n",
    "    2. For each detected object, crop the image and classify it using the classification model.\n",
    "    3. Annotate the frame with detection and classification results.\n",
    "    \"\"\"\n",
    "    \n",
    "    results_det = detection_model.single_image_detection(frame, img_path=index)\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for xyxy in results_det[\"detections\"].xyxy:\n",
    "        cropped_image = sv.crop_image(image=frame, xyxy=xyxy)\n",
    "        results_clf = classification_model.single_image_classification(cropped_image)\n",
    "        labels.append(\"{} {:.2f}\".format(results_clf[\"prediction\"], results_clf[\"confidence\"]))\n",
    "\n",
    "    annotated_frame = lab_annotator.annotate(\n",
    "        scene=box_annotator.annotate(\n",
    "            scene=frame,\n",
    "            detections=results_det[\"detections\"],\n",
    "        ),\n",
    "        detections=results_det[\"detections\"],\n",
    "        labels=labels,\n",
    "    )\n",
    "    \n",
    "    return annotated_frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d90103-192b-4c42-8bfd-d61165db5e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiempo_inicial = time.time()\n",
    "pw_utils.process_video(source_path=SOURCE_VIDEO_PATH, target_path=TARGET_VIDEO_PATH, callback=callback, target_fps=10)\n",
    "tiempo_final = time.time()\n",
    "tiempo_exec = tiempo_final - tiempo_inicial\n",
    "print(\"Execution time:\", tiempo_exec, \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python pytorch_wildlife",
   "language": "python",
   "name": "pytorch_wildlife"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
